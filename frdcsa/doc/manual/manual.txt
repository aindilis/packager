
Predator Codebase

Andrew Dougherty
     _________________________________________________________________

   Table of Contents
   1. [1]What is Predator

        [2]The Problem
        [3]Towards a Solution

   2. [4]Packaging Techniques

        [5]The Packaging Process
        [6]Examples of Architectural Components
        [7]Policy
     _________________________________________________________________

Chapter 1. What is Predator

The Problem

   Packaging software is not as complex as it seems, at least 80% of the
   time. The rules of packaging software for open source are pretty
   straight forward. Semi-automation of this process, if possible would
   aid in the packaging of existing software.
     _________________________________________________________________

Towards a Solution

   The Predator Codebase aims to do just that. The name Predator derives
   from the intended analogy between an animal that makes use of other
   animals as it's source of nourishment, and this system, which does in
   some sense the exact same thing. It is also intended to convey the
   keen adaptation of predators to the habits of their prey, the greater
   intelligence of predators, and their contingent behaviour wrt their
   prey. The point is also drawn that the functionality of other software
   nourishes the capabilities of this system. It also relates via the
   popular "Alien versus Predator" series to the Alien package which
   translates packages inbetween packaging systems. Of course the name is
   a misnomer because RADAR is really the eyes, so Digestive Track would
   be more appropriate. Obviously, the name needs to be changed. Predator
   mainly implements a system to create Debian packages out of arbitrary
   upstream codebases.
     _________________________________________________________________

Chapter 2. Packaging Techniques

The Packaging Process

   The packaging process is semi-supervised, of course, since the system
   does not begin with all of the capabilities. Of course, given the
   domain, the architecture itself is under constant elaboration and the
   development of the architecture is considered part of its learning.

   Importantly, the packages which are produced by early versions of
   Predator can later be used to facilitate capabilities. An example of
   this might be that a system for annotating and extracting text files
   could be packaged with the current, hand-written perl implementation
   of Predator, and then used to annotate files for extraction to
   increase the extent of automation.

   In addition to "learning" via programming, there are real learning
   systems employed. The first is that any action that the user takes in
   excess of Predator in order to complete a package must be
   formalizeable as an action. By documenting the various
   incompletenesses of Predator, it will be easier to add features or
   train existing learning methods to perform certain tasks.

   The basic process is currently Make like. There are several targets
   which relate to stages of packaging. Most of these steps are performed
   by existing packaging assistants, almost all of which are clearly
   specified in the Debian Maintainers Manual. Incidentally, we intend to
   formalize the process of taking documents like the Debian Developers'
   Policies and mapping the logic documented therein into code, so that,
   as best practices and online documentation are improved, updating the
   code to reflect this is an entirely auditable process. Obviously, the
   notion of process is very important here, but we have yet to find an
   appropriate system for robustly modelling processes (as opposed to
   simply hard coding them via software). This remains a major need. We
   have explored many techniques but none have been employed
   successfully, mainly on account of our own ignorance and lack of
   resources and assistance. (We have, for instance, looked at AIAI's
   I-DE and I-X, Flowdesigner, ArgoUML, AcmeStudio, OntoEdit, etc. Many
   times we have not succeeding in getting the software to even run.
   Although this could be viewed as ineptitude, it is more appropriate
   and accurate to consider it lack of resources. For instance, during
   the aforementioned testing I had not eaten in 6 days, save 2 cans of
   red kidney beans and some hotdogs one day. Secondly, this may be
   attributable to my ignorance of Software Engineering and Architecture
   practices. However, I feel that such practices are prohibitively
   difficult to learn, and seek assistance to do so. The fact that there
   are not existing tools which more or less complete the process
   indicates a certain lack of rigour in these approaches.)

   So, as the software is packaged, more cases for more software is
   elaborated. A fundamental problem is that the information does not
   come in any standard form, if at all, and therefore has to be
   estimated. One instance of this is dependency information. There are
   hybrid methods for accomplishing this. First is, every software
   package that is added, the files it contains are mapped to the desired
   metadata. Therefore, the system should be able to look in appropriate
   places, using appropriate features, to extract the desired
   information. For instance, the system should know to look in readmes,
   and to incorporate the names of known systems to approximately filter
   the readme file for dependency information. In some cases, not all the
   software is necessary, and so the annotation language, which is an
   ontology, must include relations specifying this. There are natural
   kinds of dependencies with Debian (suggets, depends, recommends,
   conflicts), etc. Using other tools like pbuilder and sbuild, it should
   be possible to do some measure of automatic testing of dependencies.
   Integrating both modes should give a head start. Lastly, data-mining
   of dependencies versus various features should allow prediction of
   what libraries are needed. All of these techniques will substantially
   reduce, and in some cases eliminate completely, the effort of the
   packaging supervisor to manually locate software. Of course it is also
   the case that the system will be invoked on each dependency, thereby,
   automatically finding and packaging (where possible) dependencies.
   This information will be used by the Machiavelli system in authoring
   persuasive letters to authors of software, based on further confidence
   building tests - such as applying question answering software to
   questions like "why isn't system X licensed open source", and user
   given approval, to write letters to the authors in the hopes of
   persuading them to relicense their software, as well as key-player
   network analysis measures to focus efforts on important cases.
   Incidentally, the persuasive letter authoring system is already
   completed.

   In conclusion, there are variety of special methods which must be
   invoked to extract information necessary to creating packages. The
   system attempts to monitor the process and activities of packagers in
   order to identify commonalties and areas for improvement. The system
   evolves both through manual coding, and automatic learning. The system
   interacts with the user, proceeding as far as possible before asking
   the user to complete some task. I forgot to mention that a planning
   process is necessary, and should be part of the larger planning
   system, and therefore able to exchange tactical information to improve
   the overall quality of automatically maintained development plans.
     _________________________________________________________________

Examples of Architectural Components

   So far, here are some of the tools which have been incorporated to
   give Predator its functionality.

   Mainly it is perl code. We have done some UML modelling of the
   process, mainly the capabilities management.

   Without a doubt, the next few steps will involve semantic web
   technologies, mainly ontology tools. The package state should be
   represented ontologically. Therefore, modules will query the ontology
   system in order to make decisions.

   The process definition is important and we have not settled on what
   this will be. Neither have we found the information extraction system,
   that is capable of handling diverse file types (such as .deb and tgz,
   pdf, etc.) Wow, that would be powerful, wouldn't it?! I'll have to
   look for that. Now, here is an example, one would simply, using the
   current state of our technology, do this :

   radar search -c "information extraction system, that is capable of
   handling diverse file types (such as .deb and tgz, pdf, etc.)"

   This command would then begin a capabilites matching search, by
   looking at the capabilities already listed in the ontology, and by
   initiating a new search with this specific criteria.

   Lastly, there are numerous Debian specific tools which do all the real
   work at this point, including dput, debmake, dh_make
   dpkg-buildpackage, cvs-buildpackage, lintian, linda, mini-dinstall,
   etc.
     _________________________________________________________________

Policy

   The policy of Predator is to collect and use applications according to
   their licenses and/or specific written agreements. A sophisticated
   license management system will be found-or-created to ensure that all
   software, indeed all data, is used in accordance with its authors
   wishes. We intend to formalize the contents of licenses so that we can
   formally prove the correctness of the software use. This license
   extraction is already done in a minor capacity, but seeing as this is
   critical, this capability will be extensively developed.

   Due to bounded resources, we are currently employing a wait and see
   approach. Our planning technology is essentially unused, i.e. there is
   not even basic goal dependency analysis. This is a major problem. If
   we got this done, we would like to then focus on other things, like
   adequately planning and developing the essential software. So, right
   now, we are simply going with an approach to create essential systems
   mainly by locating, installing and testing them, since this still
   provides greater code capability.

References

   1. file://localhost/var/lib/myfrdcsa/codebases/internal/predator/doc/manual/dump.html#AEN8
   2. file://localhost/var/lib/myfrdcsa/codebases/internal/predator/doc/manual/dump.html#overview
   3. file://localhost/var/lib/myfrdcsa/codebases/internal/predator/doc/manual/dump.html#AEN13
   4. file://localhost/var/lib/myfrdcsa/codebases/internal/predator/doc/manual/dump.html#AEN16
   5. file://localhost/var/lib/myfrdcsa/codebases/internal/predator/doc/manual/dump.html#AEN18
   6. file://localhost/var/lib/myfrdcsa/codebases/internal/predator/doc/manual/dump.html#AEN26
   7. file://localhost/var/lib/myfrdcsa/codebases/internal/predator/doc/manual/dump.html#AEN35
